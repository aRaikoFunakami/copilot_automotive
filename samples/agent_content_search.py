import asyncio
import uuid
from typing import Dict, Optional
import logging

from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent



class ChatAgent:
    """Reusable LangChain-based agent with persistent memory and thread management."""

    def __init__(self, model_name="gpt-4o", max_search_results=2):
        """
        Initialize the agent with model, search tool, and memory storage per thread.
        """
        self.model_name = model_name
        self.max_search_results = max_search_results
        self.agents: Dict[str, Dict] = {}  # Stores agents by thread_id

        # Role instructions for the agent
        self.system_prompt = """
        ã‚ãªãŸã¯å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ã‚‚ã¨ã«ã€å®Ÿåœ¨ã™ã‚‹æ˜ ç”»ã€TVãƒ‰ãƒ©ãƒã€ã¾ãŸã¯ã‚¢ãƒ‹ãƒ¡ä½œå“ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ææ¡ˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

        çµ¶å¯¾ã«æ¶ç©ºã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½œæˆã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ç¾å®Ÿã«å­˜åœ¨ã™ã‚‹ä½œå“åã ã‘ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

        Tavily ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã®æœ€æ–°ã‹ã¤ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±ã‚’å–å¾—ã§ãã¾ã™ã€‚
        Tavily ã‚’ä½¿ã£ã¦æ¤œç´¢çµæœã‚’å–å¾—ã—ãŸå ´åˆã¯ã€æ¤œç´¢çµæœã®ä¸­ã‹ã‚‰æœ€ã‚‚è©²å½“ã—ãã†ãªä½œå“ã‚’1ã¤é¸ã³ã€
        ãã®ä½œå“ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ˜ç¤ºçš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æç¤ºã—ã¦ãã ã•ã„ã€‚
        ãã®éš›ã€ã€Œã“ã®ä½œå“ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ç¢ºè¨¼ãŒãªã„ãŸã‚ã€ã‚ˆã‚Šè©³ã—ã„æƒ…å ±ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨ã„ã£ãŸè¡¨ç¾ã§ã€
        ç¢ºå®šã§ã¯ãªã„ã“ã¨ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚

        è©²å½“ã™ã‚‹ä½œå“ã‚¿ã‚¤ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã‚„ã€æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ã¨åˆ¤æ–­ã•ã‚Œã‚‹å ´åˆã¯ã€ã™ãã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‡ºã•ãšã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã•ã‚‰ã«è©³ã—ã„æƒ…å ±ï¼ˆä¾‹ï¼šå¹´ä»£ã€å›½ã€è¨€èªã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã€ãƒ—ãƒ­ãƒƒãƒˆã®è©³ç´°ãªã©ï¼‰ã‚’ä¸å¯§ã«è³ªå•ã—ã¦ãã ã•ã„ã€‚

        è³ªå•ã‚’ã™ã‚‹éš›ã«ã¯ã€ç¾æ™‚ç‚¹ã§æœ€ã‚‚è©²å½“ã—ãã†ãªå€™è£œã‚’1ä½œå“ã ã‘æŒ™ã’ã¦ãã ã•ã„ã€‚
        ãã®éš›ã€ã€Œã“ã®ä½œå“ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ç¢ºè¨¼ãŒãªã„ãŸã‚ã€ã‚ˆã‚Šè©³ã—ã„æƒ…å ±ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨ã„ã£ãŸè¡¨ç¾ã§ã€ç¢ºå®šã§ã¯ãªã„ã“ã¨ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚

        ä¼šè©±ã¯è‡ªç„¶ãªæ—¥æœ¬èªã§è¡Œã„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ä¸å¯§ã‹ã¤è¦ªåˆ‡ã«å¿œå¯¾ã—ã¦ãã ã•ã„ã€‚
        """


    def create_agent(self, thread_id: Optional[str] = None) -> str:
        """
        Create or retrieve an agent instance based on thread_id.
        If thread_id is not provided, a new one is generated.
        """
        if thread_id is None:
            thread_id = str(uuid.uuid4())  # Generate a unique thread ID

        if thread_id not in self.agents:
            memory = MemorySaver()
            model = ChatOpenAI(model_name=self.model_name)
            search = TavilySearchResults(max_results=self.max_search_results)
            tools = [search]

            agent_executor = create_react_agent(model, tools, checkpointer=memory, prompt=self.system_prompt)

            self.agents[thread_id] = {
                "executor": agent_executor,
                "memory": memory,
                "config": {"configurable": {"thread_id": thread_id}},
            }

        return thread_id  # Return thread_id so it can be reused externally

    async def run_agent(self, message: str, thread_id: str) -> str:
        """
        Run the agent asynchronously with a given message in a specified thread.
        Returns the last response from the LLM.
        """
        if thread_id not in self.agents:
            raise ValueError(f"Thread ID {thread_id} not found. Create an agent first.")

        agent_data = self.agents[thread_id]
        agent_executor = agent_data["executor"]
        config = agent_data["config"]

        response = None  # Store the last response

        async for step in agent_executor.astream(
            {"messages": [
                SystemMessage(content=self.system_prompt),
                HumanMessage(content=message),
                ]},
            config,
            stream_mode="values",
        ):
            response = step["messages"][-1].content  # Store latest response

        #logging.info(f"[Thread {thread_id}] {response}")  # Log the response
        return response  # Return the last response

    async def run_tasks(self, messages_per_thread: Dict[str, list]):
        """
        Run multiple agents concurrently in different threads.
        """
        tasks = []
        for thread_id, messages in messages_per_thread.items():
            for message in messages:
                tasks.append(self.run_agent(message, thread_id))
        results = await asyncio.gather(*tasks)  # Run all tasks concurrently
        return results  # Return all results


# --- Example Usage ---
async def main():
    chat_manager = ChatAgent()

    # Create one agent thread
    thread1 = chat_manager.create_agent("thread_123")

    while True:
        user_input = input("Enter a message (or 'exit' to quit): ")
        if user_input.lower() == "exit":
            break
        response = await chat_manager.run_agent(user_input, thread1)
        logging.info(f"ğŸ¥ Generated Title: {response}")


# Run everything inside a single event loop
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())